{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VYlUjHmmp04VjWv-xgZi13vDHDWNfsCL","timestamp":1742998133340}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### **Lab - Notebook Objective**\n","\n","The objective of this notebook is to introduce pre-trained Transformer models for language modeling. It showcases how to use these models for text generation by providing prompts and observing the generated output. Additionally, it encourages comparison between the Transformer models' performance and that of previously discussed n-gram models. The goal is to demonstrate the enhanced capabilities of Transformer models in capturing context and generating more natural and fluent text.\n","\n","<!--### Dataset:\n","\n","The section utilizes the same TinyStories dataset as before. This dataset comprises a large number of synthetically generated short stories that are suitable for basic language modeling tasks due to their simplified language and controlled vocabulary.-->\n","\n","### Libraries\n","\n","The following libraries are essential for this notebook:\n","\n","<!---   **datasets:** Used for loading and managing the TinyStories dataset. Install it using `!pip install datasets`. -->\n","-   **gemma:** Provides tools for working with Gemma language models, including loading and prompting. Install it using `!pip install gemma==3.0.0`.\n","-   **re:** Used for regular expressions in text processing.\n","-   **random:** Used for generating random numbers, particularly in text generation tasks.\n","-   **pandas:** Used for data manipulation and analysis, especially for creating and working with dataframes.\n","-   **collections:** Provides specialized data structures like `Counter` and `defaultdict`, useful for counting n-grams.\n","-   **IPython.display:** Used for displaying elements in the notebook, like clearing the output.\n","-   **keras:** A high-level neural networks API, which might be used for certain language modeling tasks.\n","\n","### Models\n","\n","In this notebook, we will use Transformer models of varying sizes: `Gemma-1B`, and `Gemma-4B`.\n","<!--\n","- TinyStories-3M, and 33M are Transformer models trained on the TinyStories English Dataset detailed in [this article](https://arxiv.org/abs/2305.07759). The suffixes 3M, and 33M represent the model sizes, with 3, and 33 million parameters, respectively.   -->\n","\n","- Gemma-1B, and 4B are Transformer models trained on a large corpus of English text comprising of web documents, code and mathematical texts. The suffixes 1B, and 4B represent the model sizes, with 1, and 4 billion parameters, respectively. More details about the model is provided in [this report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf).\n","\n","\n","The parameters of a model determine how 'big' or complex it is. The bigger the model, the more computing power is required to run it.   \n","\n","\n","\n","NOTE: For the purposes of this notebook, you will be able to load all the models on the T4 GPU hardware provided for free on Google colab. If you're using CPU, you will be able to run this activity using Gemma-1B only."],"metadata":{"id":"rErtzbUkuuci"}},{"cell_type":"markdown","metadata":{"id":"reLmASsUxIXi"},"source":["### Lab (part of the above) 1.7 Let's play with a Transformer Model now! - To comment - **Jonathan**\n","\n","\n","\n","\n","We will prompt (i.e we give a sequence of texts to) a transformer model and observe its output.\n","\n","**Things to think about for this activity**\n","\n","- As you are prompting the Transformer models, compare the quality of output with n-grams. Does the text look better? or worse? or just different? Take a close look and see what you think...\n","\n","- Does the text generated by the Transformers look more natural and fluent compared to the n-gram model?\n","\n","- Is the grammar and structure better than what you got from the n-gram model?\n","\n","- Did any part of the output seem unexpected or surprising to you?\n","\n","- Is there a difference in quality of output between the Transformer models?\n"]},{"cell_type":"markdown","source":["**Please select the type of device you are using. If unsure, please go to Runtime -> Change runtime type and select GPU, if available.**\n"],"metadata":{"id":"IMc00LreY-jM"}},{"cell_type":"code","source":["# device = 'gpu'  #@param ['cpu','gpu']"],"metadata":{"id":"xZsDtcFRZIMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gemma==3.0.0\n","!pip install datasets\n","\n","from IPython.display import clear_output\n","from datasets import load_dataset\n","import gemma as gm\n","clear_output()  # Clears the output"],"metadata":{"id":"wyrzxBZHzJqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExeLaI42XZjU"},"outputs":[],"source":["# @title keeping the code visible for now but will be hidden away\n","import os\n","import jax\n","import jax.numpy as jnp\n","import numpy as np\n","import plotly.express as px\n","from gemma import gm\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from typing import Any\n","\n","\n","def prompt_transformer_model(input_text: str, max_new_tokens: int = 10, model_name: int = 'Gemma-1B', do_sample: bool = True) -> tuple[str, np.ndarray, Any]:\n","    \"\"\"\n","    Generate text from a transformer model (Gemma ) based on the input text.\n","\n","    Args:\n","        input_text (str): The input prompt for the model.\n","        max_new_tokens (int): The maximum number of new tokens to generate.\n","        model_name (str): The name of the model to load. Supported options are 'Gemma'.\n","        do_sample (bool): Whether to use sampling for text generation (True for random sampling, False for greedy).\n","\n","    Returns:\n","        output_text (str): The generated text, including the input text and the model's output.\n","        next_token_logits (np.ndarray): Logits for the next token (probability distribution).\n","        tokenizer: The tokenizer used for encoding/decoding the text.\n","\n","    Raises:\n","        NotImplementedError: If the model_name is not recognized or supported.\n","    \"\"\"\n","\n","    assert isinstance(do_sample, bool), \"do_sample must be a boolean value.\"\n","\n","    # Process for Gemma-based models\n","    if 'Gemma' in model_name:\n","        tokenizer, model, params = load_gemma(model_name)\n","        sampler = gm.text.Sampler(\n","            model=model,\n","            params=params,\n","            tokenizer=tokenizer,\n","        )\n","\n","        if not do_sample:\n","            sampler_output_text = sampler.sample(input_text, max_new_tokens=max_new_tokens, sampling=gm.text.Greedy())\n","        else:\n","            sampler_output_text = sampler.sample(input_text, max_new_tokens=max_new_tokens, sampling=gm.text.RandomSampling())\n","\n","        # Convert the input text to tokens and apply the model to generate predictions\n","        prompt = tokenizer.encode(input_text, add_bos=True)\n","        prompt = jnp.asarray(prompt)\n","        out = model.apply(\n","            {'params': params},\n","            tokens=prompt,\n","            return_last_only=True,  # Only predict the last token\n","        )\n","\n","        next_token_logits = out.logits\n","        output_text = input_text + sampler_output_text\n","\n","    # # Process for TinyStories-based models - this will a\n","    # elif 'TinyStories' in model_name:\n","    #     tokenizer, model = load_HF(model_name)\n","    #     input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","    #     output = model.generate(input_ids, max_new_tokens=max_new_tokens, num_beams=1, do_sample=do_sample, output_scores=True, return_dict_in_generate=True)\n","\n","    #     output_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n","    #     next_token_logits = output.scores[0].flatten().numpy()\n","\n","    # If model_name is not recognized, raise an error\n","    else:\n","        raise NotImplementedError(f\"Model '{model_name}' is not recognized! Supported models: Gemma\")\n","\n","    return output_text, next_token_logits, tokenizer\n","\n","\n","def load_gemma(model_name: str = \"Gemma-1B\") -> tuple:\n","    \"\"\"\n","    Loads a Gemma model and its associated tokenizer and parameters.\n","\n","    Args:\n","        model_name (str): The name of the Gemma model to load. Options are:\n","                          \"Gemma-1B\" and \"Gemma-4B\".\n","\n","    Returns:\n","        tokenizer: Tokenizer for the specified Gemma model.\n","        model: The Gemma model.\n","        params: The parameters for the specified Gemma model.\n","\n","    Raises:\n","        ValueError: If an unsupported model name is provided.\n","    \"\"\"\n","    # Set the full GPU memory usage for JAX\n","    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n","\n","    # Initialize variables\n","    tokenizer = None\n","    model = None\n","    params = None\n","\n","    # Model loading based on model_name\n","    if model_name == \"Gemma-1B\":\n","        tokenizer = gm.text.Gemma3Tokenizer()\n","        model = gm.nn.Gemma3_1B()\n","        params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_1B_PT)\n","    elif model_name == \"Gemma-4B\":\n","        tokenizer = gm.text.Gemma3Tokenizer()\n","        model = gm.nn.Gemma3_4B()\n","        params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_4B_PT)\n","    else:\n","        raise ValueError(f\"Unsupported model name: {model_name}. Please use 'Gemma-1B' or 'Gemma-4B'.\")\n","\n","    return tokenizer, model, params\n","\n","\n","# def load_HF(model_name='TinyStories-1M'):\n","#     \"\"\"\n","#     Loads a Hugging Face model and its associated tokenizer.\n","\n","#     Args:\n","#         model_name (str): The name of the Hugging Face model to load.\n","#                           By default, it loads 'TinyStories-1M'.\n","\n","#     Returns:\n","#         tokenizer: Tokenizer for the specified Hugging Face model.\n","#         model: The Hugging Face model.\n","\n","#     Raises:\n","#         ValueError: If the model name does not contain 'TinyStories'.\n","#     \"\"\"\n","#     # Initialize variables\n","#     model = None\n","#     tokenizer = None\n","\n","#     # Load TinyStories models\n","#     if 'TinyStories' in model_name:\n","#         model = AutoModelForCausalLM.from_pretrained(f\"roneneldan/{model_name}\")\n","#         tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n","#     else:\n","#         raise ValueError(f\"Unsupported model name: {model_name}. Please ensure the model name contains 'TinyStories'.\")\n","\n","#     return tokenizer, model\n","\n","\n","\n","def plot_next_token(logits: jax.numpy.ndarray, tokenizer: Any, prompt: str, keep_top: int = 30):\n","    \"\"\"\n","    Plots the probability distribution of the next tokens given the model logits and prompt.\n","\n","    This function generates a bar plot showing the top `keep_top` tokens by probability\n","    after applying the softmax to the logits, based on the given input prompt.\n","\n","    Args:\n","        logits (jax.numpy.ndarray): The raw logits output by the model for the next token prediction.\n","        tokenizer: The tokenizer used to decode token IDs to human-readable text.\n","        prompt (str): The input prompt used to generate the next token predictions.\n","        keep_top (int): The number of top tokens to display in the plot. Default is 30.\n","\n","    Returns:\n","        None: Displays a plot showing the probability distribution of the top tokens.\n","\n","    # Function from gemma https://github.com/google-deepmind/gemma/blob/ee0d55674ecd0f921d39d22615e4e79bd49fce94/gemma/gm/text/_tokenizer.py#L249-L284\n","    \"\"\"\n","\n","    # Apply softmax to logits to get probabilities\n","    probs = jax.nn.softmax(logits)\n","\n","    # Select the top `keep_top` tokens by probability\n","    indices = jnp.argsort(probs)\n","    indices = indices[-keep_top:][::-1]  # Reverse to get highest probabilities first\n","\n","    # Get the probabilities and corresponding tokens\n","    probs = probs[indices].astype(np.float32)\n","    tokens = [repr(tokenizer.decode(i.item())) for i in indices]\n","\n","    # Create the bar plot using Plotly\n","    fig = px.bar(x=tokens, y=probs)\n","\n","    # Customize the plot layout\n","    fig.update_layout(\n","        title=f'Probability Distribution of Next Tokens given the prompt=\"{prompt}\"',\n","        xaxis_title='Tokens',\n","        yaxis_title='Probability',\n","    )\n","\n","    # Display the plot\n","    fig.show()\n"]},{"cell_type":"markdown","metadata":{"id":"gpCVDaGF95Hj"},"source":["**Your Task:**\n","1. Select the Transformer model from the `model_name` dropdown menu.\n","2. Enter a prompt of your choice using the `prompt` text field.\n","3. Click the Play button to run the cell.\n","4. Inspect the model's prediction for the next word.\n","\n","For example, if you start with the prompt: `Jide was hungry so she went looking for` the Transformer model will predict the next token. A token can be a single character (like T), a full word (like The), or a sub-word (such as Th).\n","\n","*Try running the cell several times to observe how the model responds to different prompts!*"]},{"cell_type":"markdown","source":["[Take a small pause here]\n","\n","What do you think should be the next word to follow `Jide was hungry so she went looking for` ?\n","\n","Write down your answer\n","\n","[make it free text, we can collect this and have huge dataset from student's answers]"],"metadata":{"id":"1vOeWL_P7FR3"}},{"cell_type":"markdown","source":["> Is the cell below running too slow? 🤔 Click on the `model_name` to try out a different model size. Remember, model with fewer parameters is faster!"],"metadata":{"id":"eIT5solyY3ql"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wf_FBdgRKmP","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1742920086105,"user_tz":0,"elapsed":54,"user":{"displayName":"Jonathan Pott-Negrine","userId":"03610227204231387888"}},"outputId":"8f094d90-26ef-4fce-ee05-739bf0309038","cellView":"form"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'prompt_transformer_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-957b80e69ecf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clears the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'prompt_transformer_model' is not defined"]}],"source":["model_name = \"Gemma-4B\" #@param [ \"Gemma-1B\", \"Gemma-4B\"]\n","\n","prompt = \"Jide was hungry so she went looking for\" #@param {type:\"string\"}\n","prompt = str(prompt)\n","\n","\n","output_text, next_token_logits, tokenizer = prompt_transformer_model(prompt, max_new_tokens=1, model_name=model_name)\n","clear_output() # clears the output\n","\n","print(output_text)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"8xQrrV7WEY8v"}},{"cell_type":"markdown","source":["**Visualize the probability distribution of the predicted next token**"],"metadata":{"id":"-nV9tK-R42Ja"}},{"cell_type":"markdown","metadata":{"id":"JmG3D2U6wos4"},"source":["Now that you've seen the model's prediction, let's think about the probability distribution behind the next token. The Transformer model doesn't just pick one token randomly—it actually calculates the likelihood of each possible next token, based on the context (prior words) of the prompt you provided.\n","\n","The plot below visualizes the probability distribution of the next token predicted by the language model given the prompt.  Each bar represents a different token and its height corresponds to the probability assigned to that token by the model.  \n","\n","Visualizing the probability distribution allows us to analyze the model's preferences for different token choices given the prompt.  A highly peaked distribution suggests high confidence in a single prediction, while a flatter distribution indicates greater uncertainty and a broader range of plausible next tokens.  Examining this distribution provides insights into the model's internal workings and helps us understand how it generates text, highlighting both its strengths (confident predictions) and weaknesses (uncertainty or biases)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":15,"status":"error","timestamp":1742920061728,"user":{"displayName":"Jonathan Pott-Negrine","userId":"03610227204231387888"},"user_tz":0},"id":"iNbt0z7AwnzL","outputId":"86dd752c-7d18-4b7d-ff39-2b2bd98ae58a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'plot_next_token' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f8ce569c0c83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'plot_next_token' is not defined"]}],"source":["plot_next_token(next_token_logits, tokenizer, prompt=prompt)"]},{"cell_type":"markdown","metadata":{"id":"fH5is1_RLnuS"},"source":["When you run the cell above, the model generates a probability distribution for the next token. Some tokens will have higher probabilities than others, meaning they are more likely to be chosen as the next word.\n","\n","**[Write out your observations]**\n","\n","Here are a few likely observations using the Gemma-1B model:\n","\n","1. The most probable token will usually be a common word that fits the context of the sentence (e.g., \"food\" after the prompt \"Jide was hungry so she went looking for).\n","2. The model might suggest words that seem plausible but aren't always the most expected, like \"a\" or \"something\"\n","3. You might notice some tokens have low probabilities, meaning the model considers them less likely to fit but doesn't completely rule them out like \"work\" or \"help\"\n","4. Changing the Transformer model may result in slight variations in the predicted next token, as the prediction is influenced by the model's learned weights, which are in turn determined by the dataset used for training.\n","\n","Try out different prompts and observe the probability distribution of the next token prediction!"]},{"cell_type":"markdown","metadata":{"id":"oy9BsjpKLuhE"},"source":["**Changing the context slightly**\n","\n","What happens to the probability distribution if we change the context slightly? Let's try `Jide was thirsty so she went looking for`\n","\n","*Click the Play button to run the cell below*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":8606,"status":"ok","timestamp":1742917575896,"user":{"displayName":"Annie Qurat ul ain","userId":"16820876301816973135"},"user_tz":0},"id":"TFCkKEfAMFG-","outputId":"9cb8ba30-e2fd-4879-b739-d30f336074da"},"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4256542e-66e4-467b-ae79-62e75135e686\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4256542e-66e4-467b-ae79-62e75135e686\")) {                    Plotly.newPlot(                        \"4256542e-66e4-467b-ae79-62e75135e686\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"' water'\",\"' a'\",\"' the'\",\"' some'\",\"' something'\",\"' her'\",\"' him'\",\"' his'\",\"' an'\",\"' it'\",\"' someone'\",\"' food'\",\"' one'\",\"' fresh'\",\"' clean'\",\"' drink'\",\"' drinks'\",\"' milk'\",\"' drinking'\",\"' me'\",\"' Water'\",\"' cool'\",\"' another'\",\"' juice'\",\"' tap'\",\"' any'\",\"' bottled'\",\"' tea'\",\"' H'\",\"' help'\"],\"xaxis\":\"x\",\"y\":[0.47460938,0.28710938,0.043945312,0.034179688,0.02355957,0.018432617,0.016235352,0.011169434,0.009277344,0.0067749023,0.006378174,0.0059509277,0.003616333,0.0021972656,0.0019378662,0.0017089844,0.0014190674,0.0014190674,0.0013275146,0.0013275146,0.0012512207,0.0012512207,0.00091552734,0.0008621216,0.0008621216,0.0007133484,0.000667572,0.000667572,0.000667572,0.0005912781],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Probability\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\",\"title\":{\"text\":\"Probability Distribution of Next Tokens given the prompt=\\\"Jide was thirsty so she went looking for\\\"\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('4256542e-66e4-467b-ae79-62e75135e686');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}],"source":["model_name = \"Gemma-1B\" #@param [\"Gemma-1B\", \"Gemma-4B\"]\n","\n","prompt = \"Jide was thirsty so she went looking for\" #@param {type:\"string\"}\n","prompt = str(prompt)\n","\n","\n","output_text, next_token_logits, tokenizer = prompt_transformer_model(prompt, max_new_tokens=1, model_name=model_name)\n","clear_output() # clears the output\n","\n","plot_next_token(next_token_logits, tokenizer, prompt=prompt)"]},{"cell_type":"markdown","metadata":{"id":"OxL_TBTMTI8u"},"source":["**What did you observe?**\n","\n","When running the Transformer model with prompts like \"Jide was thirsty so she went looking for\" you might notice certain patterns in the predicted next tokens. For instance, you may see drink-related words like \"water\" suggested more often. This is because the Transformer model is **context-aware** and understands that terms related to hunger and thirst tend to align with certain words—like “food” or “water”—based on the context provided by the prompt.\n","\n","\n","**Comparison Between Transformer Models**\n","\n","Different Transformer models can sometimes generate different next tokens, even for the same prompt. You might see variations in the suggestions depending on the size and training of the model you're using. Larger models, with more data and parameters, tend to generate more accurate and contextually appropriate predictions. Smaller models might be more limited in their understanding, occasionally offering less relevant or more generic predictions.\n","\n","**Transformer Models vs. N-gram Models**\n","\n","When comparing the Transformer models to traditional n-gram models, you likely noticed some key differences. N-gram models predict the next token based on a fixed window of the preceding tokens (e.g., the last two or three words). These models often struggle with longer-range dependencies or more complex sentence structures, as they only consider a limited context.\n","\n","In contrast, Transformer models consider the entire sequence of text and focus on the relationships between all tokens, not just the immediate neighbors. This makes them more flexible and accurate, especially in situations where the context stretches beyond just a few words.\n","\n","For example, when comparing outputs for the same prompt, you may see that n-gram models often fail to predict more specific words (like \"water\" or \"food\" after \"hungry\") because they don't understand the broader context as effectively. Transformer models, on the other hand, would likely generate more contextually appropriate words, like \"food\" when the prompt mentions hunger, or \"water\" when thirst is implied."]},{"cell_type":"markdown","source":["**Generating more samples**\n","\n","Now, try increasing the `num_next_tokens` to generate more texts and observe how the model responds.\n","\n","\n","*Click the Play button to run the cell below*"],"metadata":{"id":"U6dyJeR3Zz3e"}},{"cell_type":"code","source":["model_name = \"Gemma-1B\" #@param [ \"Gemma-1B\", \"Gemma-4B\"]\n","\n","prompt = \"Jide was thirsty so she went looking for\" #@param {type:\"string\"}\n","prompt = str(prompt)\n","\n","num_next_tokens = 100 #@param {type: \"number\"}\n","\n","output_text, next_word_logits, tokenizer = prompt_transformer_model(prompt, max_new_tokens=num_next_tokens, model_name=model_name)\n","clear_output() # clears the output\n","\n","print(output_text)"],"metadata":{"id":"wPVIFr68Z1vT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742917590303,"user_tz":0,"elapsed":14406,"user":{"displayName":"Annie Qurat ul ain","userId":"16820876301816973135"}},"outputId":"ab725c43-2c64-487c-abbf-e7cd5ba377c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jide was thirsty so she went looking for a cool water which is located near her house. She went and took a good drink before she started her search. After she had taken her drink she went back home with her new found water. As she was coming her way another woman was also thirsty so she too went in search of a cool and refreshing water. While she was looking for water she saw a man trying to stop a woman who had fallen in the pool. The woman was bleeding from her arm. She went over to the man and\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"krg0AbIlJ_Zb"}},{"cell_type":"markdown","source":["**Language Models as Stochastic Parrots**\n","\n","When you ran the cell above multiple times, what did you notice?\n","\n","- Did you observe stereotypical outputs like Jide carrying a pot full of mud?\n","- or perhaps the model switched gender from female to male pronoun mid sentence?\n","[Revise this]\n","- or it defaulted to common names seen frequently in its training data (like 'Lily', 'Jack', 'Jill', etc., in models trained only on datasets like TinyStories)?\n","\n","Ultimately, language models are adept at predicting the next token, but they closely follow the distribution of their training data. If the model is trained on biased data, it will produce biased outputs. Similarly, if it's trained on data scraped from the entire internet, it will reflect the dominant texts and perspectives found there, often sidelining less common viewpoints and cultures. This can lead to the model reinforcing stereotypes, such as certain professions being associated with specific genders.\n","\n","In upcoming modules, we'll revisit these issues and explore ways to better align language model outputs with humane values and preferences."],"metadata":{"id":"Fggmt1p4Xn1y"}},{"cell_type":"markdown","source":["**The Output Above Changes Every Time You Ran the Cell, Right?**\n","\n","You likely noticed that the output of the Transformer model changes each time you run the cell above, even with the same prompt. This is because the model uses a probability distribution to pick the next token, which introduces a level of stochasticity (randomness) into the prediction. This is similar to what you saw in the n-gram models, where the next word isn't always the same due to the model sampling from a set of possibilities.\n","\n","This variability helps the Transformer model generate more diverse and creative outputs.\n","\n","**Controlling the Model's Output**\n","\n","If you want the model to always pick the token with the **highest probability** (meaning the most likely next token), you can set the variable `do_sample=False`. This will make the model more deterministic and return the most probable token each time.\n","\n","Here's how you can do that:\n","\n","```python\n","prompt_transformer_model(prompt, max_new_tokens=num_next_tokens, model_name=model_name, do_sample=False)\n","```\n","\n","With this setting, the output will be consistent across multiple runs for the same prompt, as it always selects the most probable token.\n","\n","**Sampling Mode (Default: `do_sample=True`)**\n","\n","By default, when `do_sample=True`, the model samples from the probability distribution, which introduces randomness and results in more varied and creative outputs. This is helpful when you want the model to explore a range of possible continuations for a prompt, rather than sticking strictly to the most likely outcome."],"metadata":{"id":"aGF0ds-NiBYt"}},{"cell_type":"markdown","source":["*Run the cell below multiple times and observe the result.*"],"metadata":{"id":"-GPktHKqhCfZ"}},{"cell_type":"code","source":["model_name = \"Gemma-1B\" #@param [\"Gemma-1B\", \"Gemma-4B\"]\n","\n","prompt = \"Jide was thirsty so she went looking for\" #@param {type:\"string\"}\n","\n","num_next_tokens = 100 #@param {type: \"number\"}\n","\n","output_text, next_word_logits, tokenizer = prompt_transformer_model(prompt, max_new_tokens=num_next_tokens, model_name=model_name, do_sample=False)\n","clear_output() # clears the output\n","\n","print(output_text)"],"metadata":{"id":"Rv7PzJFQdXiP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bcec023-cea2-48a2-fc99-be0d08de0dba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:[process=0][thread=MainThread] No metadata found for any process_index, checkpoint_dir=gs://gemma-data/checkpoints/gemma3-1b-pt. time elapsed=0.03984546661376953 seconds. If the checkpoint does not contain jax.Array then it is expected. If checkpoint contains jax.Array then it should lead to an error eventually; if no error is raised then it is a bug.\n"]}]},{"cell_type":"markdown","source":["*Running the cell above multiple times should return the same output!*"],"metadata":{"id":"Mst3PmFqdiW-"}},{"cell_type":"markdown","source":["**Balancing creativity and consistency**\n","\n","Sampling from a probability distribution allows the Transformer model to explore a range of possible next tokens, fostering creativity and generating diverse outputs. This approach contrasts with always picking the token with the highest probability, which focuses on the most likely next token, as you have seen above.\n","\n","Different applications require different settings for this balance. For creative tasks, such as generating stories, sampling from the probability distribution is ideal because it allows the model to explore various possibilities and produce more imaginative results. On the other hand, in sensitive domains like healthcare, where accuracy, consistency, and reliability are critical, it's better to choose the token with the highest probability, as this reflects the model's highest certainty and minimizes the risk of errors."],"metadata":{"id":"MV6Qd59qFuq-"}},{"cell_type":"markdown","source":[" `End of second notebook `\n","\n"],"metadata":{"id":"kEW2iL2z1Clp"}}]}